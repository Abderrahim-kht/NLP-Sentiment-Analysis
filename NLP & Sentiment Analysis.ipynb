{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TF-IDF and N-Grams__\n",
    "\n",
    "The goal of this project was to predict the sentiment of an IMDB movie review using a binary classification system. The dataset was part of the Bag of Words Meets Bag of Popcorn Competition.\n",
    "\n",
    "Model Accuracy: 0.89532\n",
    "\n",
    "Bag of Words & TF-IDF\n",
    "A Bag of Words (BoW) model is a simple algorithm used in Natural Language Processing. It simply counts the number of times a word appears in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Generation using TF-IDF__\n",
    "\n",
    "In Term Frequency(TF), you just count the number of words occurred in each document. The main issue with this Term Frequency is that it will give more weight to longer documents. Term frequency is basically the output of the BoW model.\n",
    "\n",
    "IDF(Inverse Document Frequency) measures the amount of information a given word provides across the document. IDF is the logarithmically scaled inverse ratio of the number of documents that contain the word and the total number of documents.\n",
    "\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. Word with high tf-idf in a document, it is most of the times occurred in given documents and must be absent in the other documents. So the words must be a signature word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the project, I used two separate TF-IDF vectorizers and merged them into a single bag of words.\n",
    "\n",
    "-  The first vectorizer (word_vectorizer) analyzed complete words.\n",
    "-  The second vectorizer (char_vectorizer) analyzed the frequency of character n-grams. An n-gram is a continous sequence of n items from a document. Using Trigrams (N-gram size = 3) yielded a high predictive score.\n",
    "\n",
    "Lastly, we used a Logistic Regression to predict the sentiment attached to each review. The hyperparameters of the model were tuned using a validation dataset prior to training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Required Libraries and Reading the Data into Python\n",
    "First, we need to load the required libraries and read the data into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, delimiter=\"\\t\")\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\")\n",
    "\n",
    "train_text = train['review']\n",
    "test_text = test['review']\n",
    "y = train['sentiment']\n",
    "\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TF-IDF Vectorizers__\n",
    "\n",
    "First, we convert the reviews into a Bag of Words using the TF-IDF vectorizer for words and for character trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', sublinear_tf=True, strip_accents='unicode', \n",
    "                                  stop_words='english', ngram_range=(1, 1), max_features=10000)\n",
    "word_vectorizer.fit(train_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(analyzer='char', sublinear_tf=True, strip_accents='unicode', \n",
    "                                  stop_words='english', ngram_range=(1, 3), max_features=50000)\n",
    "char_vectorizer.fit(train_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = hstack([train_word_features, train_char_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperparameter Tuning of Logistic Regression__\n",
    "\n",
    "Since there are multiple hyperparameters to tune in the XGBoost model, we will use the GridSearchCV function of Sklearn to determine the optimal hyperparameter values. Next, I used the train_test_split function to generate a validation set and find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khata\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\khata\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took 323.94 seconds to complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'solver': 'saga'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated Score of the Best Estimator: 0.888\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_features, y,test_size=0.3 ,random_state=1234)\n",
    "\n",
    "lr_model = LogisticRegression(random_state=1234)\n",
    "param_dict = {'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "             'solver': ['sag', 'lbfgs', 'saga']}\n",
    "\n",
    "start = time()\n",
    "grid_search = GridSearchCV(lr_model, param_dict)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"GridSearch took %.2f seconds to complete.\" % (time()-start))\n",
    "display(grid_search.best_params_)\n",
    "print(\"Cross-Validated Score of the Best Estimator: %.3f\" % grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our model does on the validation dataset and where any misclassifications occur.\n",
    "\n",
    "We have several metrics available for classification accuracy, including a confusion matrix and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3367  398]\n",
      " [ 368 3367]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      3765\n",
      "           1       0.89      0.90      0.90      3735\n",
      "\n",
      "    accuracy                           0.90      7500\n",
      "   macro avg       0.90      0.90      0.90      7500\n",
      "weighted avg       0.90      0.90      0.90      7500\n",
      "\n",
      "Accuracy Score: 0.898\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression(C=1, solver ='saga')\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds=lr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, lr_preds))\n",
    "print(classification_report(y_test, lr_preds))\n",
    "print(\"Accuracy Score: %.3f\" % accuracy_score(y_test, lr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of false positives (FP = 366) is similar to the number of false negatives (FN = 399), suggesting that our model is not biased towards either specificity nor sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation (TF-IDF)\n",
    "\n",
    "Model Building and Evaluation (TF-IDF)\n",
    "Let's build the Text Classification Model using TF-IDF.\n",
    "\n",
    "First, import the LogisticRegression module using the tuned hyperparameters.\n",
    "Then, fit your model on a train set using fit() and perform prediction on the test set using predict()\n",
    "Format predictions for submission to Kaggle Competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', sublinear_tf=True, strip_accents='unicode', \n",
    "                                  stop_words='english', ngram_range=(1, 1), max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(analyzer='char', sublinear_tf=True, strip_accents='unicode', \n",
    "                                  stop_words='english', ngram_range=(1, 3), max_features=50000)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(C=1,solver='saga')\n",
    "lr.fit(train_features,y)\n",
    "final_preds=lr.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making submission for Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment'] = final_preds\n",
    "test = test[['id', 'sentiment']]\n",
    "test.to_csv('Submissionn.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
